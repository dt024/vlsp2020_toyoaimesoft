{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ElsnSNUridI"
   },
   "source": [
    "# Install packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0NmMdkZO8R6q",
    "outputId": "87c01804-8533-47fa-e889-38b80379ab12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 1.4MB 13.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 890kB 56.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.9MB 66.3MB/s \n",
      "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 716kB 14.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 81kB 11.4MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.1MB 32.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 174kB 60.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 36.7MB 79kB/s \n",
      "\u001b[K     |████████████████████████████████| 102kB 17.2MB/s \n",
      "\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "# !pip install -q tensorflow==2.2-rc1\n",
    "!pip install -q tf-models-official==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZ_8mMli_1sD",
    "outputId": "6627ffc2-995d-45c3-fed6-8d7436a97dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-lr-multiplier\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/78/0eed4862a7274fb491b50881dd2f0dac996ff5774dc4a30c4b628fb78b25/keras-lr-multiplier-0.8.0.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-lr-multiplier) (1.18.5)\n",
      "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-lr-multiplier) (2.4.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-lr-multiplier) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-lr-multiplier) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-lr-multiplier) (2.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras-lr-multiplier) (1.15.0)\n",
      "Building wheels for collected packages: keras-lr-multiplier\n",
      "  Building wheel for keras-lr-multiplier (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-lr-multiplier: filename=keras_lr_multiplier-0.8.0-cp36-none-any.whl size=5719 sha256=e5f27e0e48ac9bc2f1ad461a15410b693e32e110d5a7bd331a00a8b7f10ad1aa\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/a5/a4/340d5432bced221b2bcca324e3257239784dd1220ab7c786e9\n",
      "Successfully built keras-lr-multiplier\n",
      "Installing collected packages: keras-lr-multiplier\n",
      "Successfully installed keras-lr-multiplier-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-lr-multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQUy9Tat2EF_"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4F_Bi5dp3LsP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFAutoModel, AutoTokenizer, TFBertForSequenceClassification,AutoConfig\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D,Average,Dot, Dense, Input, GlobalAveragePooling1D, BatchNormalization, Activation, Concatenate, Flatten, Dropout, Conv1D, MaxPooling1D, Add, Lambda, GlobalAveragePooling2D, Reshape, RepeatVector, UpSampling1D \n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from official import nlp\n",
    "import official.nlp.optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2H2gCoL4b-_"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "DUto8PzE4dk8"
   },
   "outputs": [],
   "source": [
    "base_dir    = '/vlsp2020'\n",
    "train_path  = os.path.join(base_dir, 'train_rev1_news.csv')\n",
    "val_path    = os.path.join(base_dir, 'dev_rev1_news.csv')\n",
    "test_path    = os.path.join(base_dir, 'final_news1.csv')\n",
    "img_train_path = os.path.join(base_dir, 'train-image-299.npy')\n",
    "img_val_path = os.path.join(base_dir, 'val-image-299.npy')\n",
    "img_test_path = os.path.join(base_dir, 'final-image-299.npy')\n",
    "\n",
    "MAX_LENGTH  = 256\n",
    "MODEL       = 'NlpHUST/vibert4news-base-cased'\n",
    "MODEL_NAME  = 'vibert4news-model1'\n",
    "N_LABELS    = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0QV2ntMpuBP"
   },
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_UkeC7SG2krJ",
    "outputId": "c5c33279-2f44-4445-93f6-39e1838c6cfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3933, 26)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3933 entries, 0 to 3932\n",
      "Data columns (total 26 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   index               3933 non-null   int64  \n",
      " 1   id                  3933 non-null   int64  \n",
      " 2   user_name           3933 non-null   object \n",
      " 3   post_message        3933 non-null   object \n",
      " 4   timestamp_post      3933 non-null   float64\n",
      " 5   num_like_post       3933 non-null   float64\n",
      " 6   num_comment_post    3933 non-null   float64\n",
      " 7   num_share_post      3933 non-null   float64\n",
      " 8   label               3933 non-null   int64  \n",
      " 9   cleaned_text        3933 non-null   object \n",
      " 10  count_chars         3933 non-null   int64  \n",
      " 11  count_words         3933 non-null   int64  \n",
      " 12  count_questionmark  3933 non-null   int64  \n",
      " 13  count_exclaimmark   3933 non-null   int64  \n",
      " 14  numHashtags         3933 non-null   int64  \n",
      " 15  numUrls             3933 non-null   int64  \n",
      " 16  post_month          3933 non-null   int64  \n",
      " 17  post_year           3933 non-null   int64  \n",
      " 18  post_day            3933 non-null   int64  \n",
      " 19  post_hour           3933 non-null   int64  \n",
      " 20  post_weekday        3933 non-null   int64  \n",
      " 21  cnt_fake            3933 non-null   float64\n",
      " 22  cnt_nonfake         3933 non-null   float64\n",
      " 23  ratio               3933 non-null   float64\n",
      " 24  has_title           3933 non-null   int64  \n",
      " 25  has_image           3933 non-null   int64  \n",
      "dtypes: float64(7), int64(16), object(3)\n",
      "memory usage: 799.0+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>post_message</th>\n",
       "      <th>timestamp_post</th>\n",
       "      <th>num_like_post</th>\n",
       "      <th>num_comment_post</th>\n",
       "      <th>num_share_post</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>count_chars</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_questionmark</th>\n",
       "      <th>count_exclaimmark</th>\n",
       "      <th>numHashtags</th>\n",
       "      <th>numUrls</th>\n",
       "      <th>post_month</th>\n",
       "      <th>post_year</th>\n",
       "      <th>post_day</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>cnt_fake</th>\n",
       "      <th>cnt_nonfake</th>\n",
       "      <th>ratio</th>\n",
       "      <th>has_title</th>\n",
       "      <th>has_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>99ed0ae1e3149e05968a0b27b4317f1d</td>\n",
       "      <td>Chị Phạm Vũ Tâm An -Trung tâm tư vấn tâm lý Ph...</td>\n",
       "      <td>1.590871e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Chị Phạm Vũ Tâm An -Trung tâm tư vấn tâm lý Ph...</td>\n",
       "      <td>157</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1755</td>\n",
       "      <td>1756</td>\n",
       "      <td>a7ed39c9ac4f4eaf9534651d39c03849</td>\n",
       "      <td>Họ chỉ là những người bình thường đang làm côn...</td>\n",
       "      <td>1.585020e+09</td>\n",
       "      <td>2089</td>\n",
       "      <td>50</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>Họ chỉ là những người bình thường đang làm côn...</td>\n",
       "      <td>222</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3607</td>\n",
       "      <td>3608</td>\n",
       "      <td>7c0129565612ac73129fdfa652575e48</td>\n",
       "      <td># **XÚC ĐỘNG TỰ HÀO VÀ NGƯỠNG MỘ BIẾT ƠN VỀ MỘ...</td>\n",
       "      <td>1.589860e+09</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td># **xúc động tự hào và ngưỡng mộ biết ơn về mộ...</td>\n",
       "      <td>6433</td>\n",
       "      <td>1454</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1823</td>\n",
       "      <td>1824</td>\n",
       "      <td>d5c03b35abe8da9485f462af36879c8c</td>\n",
       "      <td>Tài xế ô tô siêu sang chống đối CSGT, lái xe h...</td>\n",
       "      <td>1.584770e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Tài xế ô tô siêu sang chống đối csgt, lái xe h...</td>\n",
       "      <td>661</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3289</td>\n",
       "      <td>3290</td>\n",
       "      <td>ae42a3a4976579103ceaeb5d7b78909b</td>\n",
       "      <td># **SAU 40 NĂM, 1.600 CHIẾN SĨ ANH HÙNG ĐƯỢC N...</td>\n",
       "      <td>1.587979e+09</td>\n",
       "      <td>224</td>\n",
       "      <td>76</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td># **sau 40 năm, 1.600 chiến sĩ anh hùng được n...</td>\n",
       "      <td>1093</td>\n",
       "      <td>224</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2020</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    id  ... has_title has_image\n",
       "0   2005  2006  ...         0         0\n",
       "1   1755  1756  ...         0         0\n",
       "2   3607  3608  ...         1         0\n",
       "3   1823  1824  ...         1         1\n",
       "4   3289  3290  ...         1         0\n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv(train_path)\n",
    "print(df_train.shape)\n",
    "print(df_train.info())\n",
    "df_train[['num_like_post','num_comment_post','num_share_post']] = df_train[['num_like_post','num_comment_post','num_share_post']].astype(int)\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "blqIvQaQncdJ",
    "outputId": "2e05358e-86c6-4220-f59c-55e96f30af47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438, 26)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 438 entries, 0 to 437\n",
      "Data columns (total 26 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   index               438 non-null    int64  \n",
      " 1   id                  438 non-null    int64  \n",
      " 2   user_name           438 non-null    object \n",
      " 3   post_message        438 non-null    object \n",
      " 4   timestamp_post      438 non-null    float64\n",
      " 5   num_like_post       438 non-null    float64\n",
      " 6   num_comment_post    438 non-null    float64\n",
      " 7   num_share_post      438 non-null    float64\n",
      " 8   label               438 non-null    int64  \n",
      " 9   cleaned_text        438 non-null    object \n",
      " 10  count_chars         438 non-null    int64  \n",
      " 11  count_words         438 non-null    int64  \n",
      " 12  count_questionmark  438 non-null    int64  \n",
      " 13  count_exclaimmark   438 non-null    int64  \n",
      " 14  numHashtags         438 non-null    int64  \n",
      " 15  numUrls             438 non-null    int64  \n",
      " 16  post_month          438 non-null    int64  \n",
      " 17  post_year           438 non-null    int64  \n",
      " 18  post_day            438 non-null    int64  \n",
      " 19  post_hour           438 non-null    int64  \n",
      " 20  post_weekday        438 non-null    int64  \n",
      " 21  cnt_fake            438 non-null    float64\n",
      " 22  cnt_nonfake         438 non-null    float64\n",
      " 23  ratio               438 non-null    float64\n",
      " 24  has_title           438 non-null    int64  \n",
      " 25  has_image           438 non-null    int64  \n",
      "dtypes: float64(7), int64(16), object(3)\n",
      "memory usage: 89.1+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>post_message</th>\n",
       "      <th>timestamp_post</th>\n",
       "      <th>num_like_post</th>\n",
       "      <th>num_comment_post</th>\n",
       "      <th>num_share_post</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>count_chars</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_questionmark</th>\n",
       "      <th>count_exclaimmark</th>\n",
       "      <th>numHashtags</th>\n",
       "      <th>numUrls</th>\n",
       "      <th>post_month</th>\n",
       "      <th>post_year</th>\n",
       "      <th>post_day</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>cnt_fake</th>\n",
       "      <th>cnt_nonfake</th>\n",
       "      <th>ratio</th>\n",
       "      <th>has_title</th>\n",
       "      <th>has_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3981</td>\n",
       "      <td>3982</td>\n",
       "      <td>4ebf9484c0e0600b46c4ccf7e315e87e</td>\n",
       "      <td>Một trong những vấn đề được nhiều đại biểu qua...</td>\n",
       "      <td>1.592406e+09</td>\n",
       "      <td>153</td>\n",
       "      <td>226</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>Một trong những vấn đề được nhiều đại biểu qua...</td>\n",
       "      <td>443</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3903</td>\n",
       "      <td>3904</td>\n",
       "      <td>808e278b22ec6b96f2faf7447d10cd8e</td>\n",
       "      <td>Giá cổ phiếu suy giảm khiến tỷ phú giàu nhất b...</td>\n",
       "      <td>1.584736e+09</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Giá cổ phiếu suy giảm khiến tỷ phú giàu nhất b...</td>\n",
       "      <td>180</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>5e631179c3cc2a90a3afd12b08819770</td>\n",
       "      <td>Lưu ý lưu ý 15/5 - 14/6 🙂🙂🙂\\nTrong thời gian r...</td>\n",
       "      <td>1.588824e+09</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Lưu ý lưu ý 15/5 - 14/6 🙂🙂🙂 Trong thời gian ra...</td>\n",
       "      <td>219</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4262</td>\n",
       "      <td>4263</td>\n",
       "      <td>2a205aa672d1d1e9132029d5a163ce62</td>\n",
       "      <td>Với chiêu lừa \"quái dị\" HTX Ngọc Đăng ở Phú Th...</td>\n",
       "      <td>1.590836e+09</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Với chiêu lừa \"quái dị\" htx Ngọc Đăng ở Phú Th...</td>\n",
       "      <td>357</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2304</td>\n",
       "      <td>2305</td>\n",
       "      <td>8da2ab7d30849156b2105a21b2fb33cf</td>\n",
       "      <td>Tổ chức đám cưới linh đình cho con trai, gia đ...</td>\n",
       "      <td>1.583194e+09</td>\n",
       "      <td>589</td>\n",
       "      <td>41</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>Tổ chức đám cưới linh đình cho con trai, gia đ...</td>\n",
       "      <td>180</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    id  ... has_title has_image\n",
       "0   3981  3982  ...         0         0\n",
       "1   3903  3904  ...         0         0\n",
       "2     25    26  ...         0         0\n",
       "3   4262  4263  ...         0         0\n",
       "4   2304  2305  ...         0         0\n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_val = pd.read_csv(val_path)\n",
    "print(df_val.shape)\n",
    "print(df_val.info())\n",
    "df_val[['num_like_post','num_comment_post','num_share_post']] = df_val[['num_like_post','num_comment_post','num_share_post']].astype(int)\n",
    "display(df_val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "train_sent      = df_train.cleaned_text.values\n",
    "train_labels    = df_train.label.values\n",
    "val_sent        = df_val.cleaned_text.values\n",
    "val_labels      = df_val.label.values \n",
    "#test_sent        = df_test.cleaned_text.values\n",
    "#test_labels      = df_test.label.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hraBqZt23Q7n",
    "outputId": "9ca6e835-4400-44ac-aa8a-fd03a81ba749"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3933, 299, 299, 3), (438, 299, 299, 3), (1646, 299, 299, 3))"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Image\n",
    "img_train = np.load(img_train_path)\n",
    "# img_train = np.delete(img_train,duplicated,0)\n",
    "img_val = np.load(img_val_path)\n",
    "img_test = np.load(img_test_path)\n",
    "img_train.shape, img_val.shape, img_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "KG1x57Sv-k5X"
   },
   "outputs": [],
   "source": [
    "img_train = img_train.astype('float32')\n",
    "img_val = img_val.astype('float32')\n",
    "img_test = img_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6XvCkqT7wxO",
    "outputId": "b8e2ece3-77e6-450f-ff6a-6d1d8f3157f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3933 3933\n",
      "438 438\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sent), len(train_labels))\n",
    "print(len(val_sent), len(val_labels))\n",
    "#print(len(test_sent), len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex5O1eV-Pfct"
   },
   "source": [
    "# Tokenization & Input Formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z474sSC6oe7A",
    "outputId": "457ae462-35b1-4267-b0d3-f8523f68041a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLIbudgfh6F0",
    "outputId": "ba97502b-6b47-40ad-8a5f-f5e741bb4fd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Chị Phạm_Vũ_Tâm_An - Trung_tâm tư_vấn tâm_lý Phúc_An cho_hay chị từng tư_vấn tâm_lý cho nhiều khách_hàng gặp vấn_đề xung_đột với bạn_đời vì thói_quen ăn_uống . \n",
      "Tokenized:  ['Chị', 'Phạm_@@', 'Vũ_@@', 'Tâm_An', '-', 'Trung_tâm', 'tư_vấn', 'tâm_lý', 'Phúc_@@', 'An', 'cho_hay', 'chị', 'từng', 'tư_vấn', 'tâm_lý', 'cho', 'nhiều', 'khách_hàng', 'gặp', 'vấn_đề', 'xung_đột', 'với', 'bạn_đời', 'vì', 'thói_quen', 'ăn_uống', '.']\n",
      "Token IDs:  [1108, 2016, 2327, 42098, 31, 664, 1408, 1523, 5197, 2234, 897, 213, 150, 1408, 1523, 13, 36, 478, 243, 220, 2463, 15, 8146, 90, 2223, 2086, 5]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', train_sent[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(train_sent[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sent[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaunFPWIFZzJ"
   },
   "source": [
    "## Tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjademTSD0VA"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2sOyU4n23Br",
    "outputId": "b3c34a82-36a4-49dc-8ca1-05308a4dcd80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'id', 'user_name', 'post_message', 'timestamp_post',\n",
       "       'num_like_post', 'num_comment_post', 'num_share_post', 'label',\n",
       "       'cleaned_text', 'count_chars', 'count_words', 'count_questionmark',\n",
       "       'count_exclaimmark', 'numHashtags', 'numUrls', 'post_month',\n",
       "       'post_year', 'post_day', 'post_hour', 'post_weekday', 'cnt_fake',\n",
       "       'cnt_nonfake', 'ratio', 'has_title', 'has_image'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "conaoCbXw0QF",
    "outputId": "0d6a82e7-1c93-44e8-d5ef-845e3c52b1b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns = ['num_like_post', 'num_comment_post', 'num_share_post', \n",
    "                   'count_chars', 'count_words', 'count_questionmark',\n",
    "                   'count_exclaimmark', 'numHashtags', 'numUrls', 'post_month',\n",
    "                   'post_day', 'post_hour', 'post_weekday', 'cnt_fake',\n",
    "                   'cnt_nonfake', 'ratio', 'has_image']\n",
    "FEATURES_NUM = len(feature_columns)\n",
    "FEATURES_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "qGMxQ7sJdehJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[feature_columns[:-2]])\n",
    "\n",
    "df_train[feature_columns[:-2]] = scaler.transform(df_train[feature_columns[:-2]])\n",
    "df_val[feature_columns[:-2]] = scaler.transform(df_val[feature_columns[:-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "xPmH-kZtxlRh"
   },
   "outputs": [],
   "source": [
    "train_features = df_train[feature_columns]\n",
    "#process_features(train_features)\n",
    "train_features = train_features.astype('float32')\n",
    "\n",
    "val_features = df_val[feature_columns]\n",
    "#process_features(val_features)\n",
    "val_features = val_features.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bBdb3pt8LuQ",
    "outputId": "0cf8a90f-10fa-42bf-e905-1634fc9c26a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3933 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "100%|██████████| 3933/3933 [00:02<00:00, 1672.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3933, 256), (3933, 256), (3933,), (3933, 17))"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids       = []\n",
    "attention_masks = []\n",
    "features = []\n",
    "\n",
    "for sent in tqdm(train_sent):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = MAX_LENGTH,         \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'np',   \n",
    "                        truncation = True,\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "for id,_ in enumerate(train_features['count_chars']):\n",
    "  f = train_features.iloc[id].values\n",
    "  features.append(f)\n",
    "\n",
    "  \n",
    "id_train        = np.concatenate(input_ids)\n",
    "mask_train      = np.concatenate(attention_masks)\n",
    "feature_train   = np.array(features)\n",
    "y_train         = train_labels \n",
    "id_train.shape, mask_train.shape, y_train.shape, feature_train.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neekd6auD2Zf"
   },
   "source": [
    "### Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GEgLpFVlo1Z-",
    "outputId": "70dc8ef9-81e3-4e1e-ae3d-2a50eebce6aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "100%|██████████| 438/438 [00:00<00:00, 1729.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((438, 256), (438, 256), (438,), (438, 17))"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids       = []\n",
    "attention_masks = []\n",
    "features        = []\n",
    "\n",
    "for sent in tqdm(val_sent):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = MAX_LENGTH,         \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'np',   \n",
    "                        truncation = True,\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "for id,_ in enumerate(val_features['count_chars']):\n",
    "  f = val_features.iloc[id].values\n",
    "  features.append(f)\n",
    "\n",
    "\n",
    "id_val          = np.concatenate(input_ids)\n",
    "mask_val        = np.concatenate(attention_masks)\n",
    "feature_val     = np.array(features)\n",
    "y_val           = val_labels\n",
    "id_val.shape, mask_val.shape, y_val.shape, feature_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvidOCpCAtQ4"
   },
   "source": [
    "## Create iterator for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "XGUqOCtgqGhP"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE      = 16 \n",
    "\n",
    "X_train         = [\n",
    "    id_train,\n",
    "    mask_train,\n",
    "    feature_train,\n",
    "    img_train\n",
    "]\n",
    "X_val           = [\n",
    "    id_val,\n",
    "    mask_val,\n",
    "    feature_val,\n",
    "    img_val  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bwa6Rts-02-"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "yvdfkSl1YZto"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19                                  \n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input                 \n",
    "\n",
    "def create_cnn(input_shape):\n",
    "  inputs = Input(shape=input_shape)\n",
    "  x = Lambda(preprocess_input)(inputs)\n",
    "  # load the VGG16 network, ensuring the head FC layer sets are left\n",
    "  # off                                                                           \n",
    "  baseModel = VGG19(weights=\"imagenet\", include_top=False, input_tensor=x)\n",
    "\n",
    "  # construct the head of the model that will be placed on top of the\n",
    "  # the base model                                           \n",
    "  headModel = baseModel.output  \n",
    "  \n",
    "  ###Attention\n",
    "  headModel = Reshape((-1,headModel.shape[-1]))(headModel)\n",
    "\n",
    "  ###Non-attention\n",
    "  # headModel = GlobalAveragePooling2D()(headModel)                                 \n",
    "  # headModel = Dense(512)(headModel)\n",
    "  # headModel = BatchNormalization()(headModel)\n",
    "  # headModel = Activation(\"relu\")(headModel)\n",
    "  # headModel = Dropout(0.2)(headModel)\n",
    "\n",
    "  # headModel = Dense(512)(headModel)\n",
    "  # headModel = BatchNormalization()(headModel)\n",
    "  # headModel = Activation(\"relu\")(headModel)\n",
    "  # headModel = Dropout(0.2)(headModel)\n",
    "\n",
    "  # headModel = Dense(512)(headModel)\n",
    "  # headModel = BatchNormalization()(headModel)\n",
    "  # headModel = Activation(\"relu\")(headModel)\n",
    "  # headModel = Dropout(0.2)(headModel)\n",
    "\n",
    "  model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "  # loop over all layers in the base model and freeze them so they will\n",
    "  # *not* be updated during the first training process\n",
    "  #for layer in baseModel.layers[:-8]:\n",
    "  #  layer.trainable = False\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "D6f-qYCpPVc4"
   },
   "outputs": [],
   "source": [
    "def create_model1(transformer, max_len=256, feature_num=17):\n",
    "    merge = []\n",
    "\n",
    "    input_ids           = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask      = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "    extra_features      = Input(shape=(feature_num,), dtype=tf.int32, name='extra_features')\n",
    "    image               = create_cnn((299,299,3))\n",
    "    sequence_output     = transformer(input_ids, \n",
    "                                  attention_mask=attention_mask)[0]\n",
    "   #cls_token           = sequence_output[:, 0, :]\n",
    "    \n",
    "    dense               = Dense(512)(extra_features)\n",
    "    bn                  = BatchNormalization()(dense)\n",
    "    dense               = Activation(\"relu\")(bn)\n",
    "    #merge.append(cls_token)\n",
    "    merge.append(dense)\n",
    "  \n",
    "   # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5]\n",
    "    size_pool = 5\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=256, kernel_size=filter_size)(sequence_output)\n",
    "        l_conv = BatchNormalization()(l_conv)\n",
    "        l_conv = Activation(\"relu\")(l_conv)\n",
    "        l_pool = MaxPooling1D(pool_size=max_len-filter_size+1)(l_conv)\n",
    "        # merge.append(Flatten()(l_pool))\n",
    "        convs.append(Flatten()(l_pool))\n",
    "\n",
    "    text = Concatenate(axis=1)(convs)\n",
    "    # text = Dropout(0.2)(text)\n",
    "    text = Dense(512)(text)\n",
    "    text = BatchNormalization()(text)\n",
    "    text = Activation(\"relu\")(text)                  #change to tanh?\n",
    "    text = Dropout(0.2)(text)\n",
    "    merge.append(text)\n",
    "    ##With attention\n",
    "    img = Dense(512)(image.output) #change to tanh? \n",
    "    img = BatchNormalization()(img)\n",
    "    img = Activation('tanh')(img)\n",
    "\n",
    "    #text = RepeatVector(img.shape[1])(text)\n",
    "    #attention = Add()([text, img])\n",
    "    #attention = Dense(512,activation='relu')(attention)\n",
    "    #attention = Dense(1)(attention)\n",
    "    attention = Dot(axes=(1,2))([text, img])\n",
    "    attention = Activation(\"softmax\")(attention)\n",
    "    att_img = Dot(axes=(1,1))([attention, image.output])\n",
    "    merge.append(Flatten()(att_img))\n",
    "    ##Without attention\n",
    "    #merge.append(image.output)\n",
    "\n",
    "    #l_merge             = Concatenate(axis=1)(merge)\n",
    "    l_merge             = Average()(merge)   \n",
    "    out                 = Dense(N_LABELS, activation='sigmoid')(l_merge) \n",
    "    model               = Model(inputs=[input_ids, attention_mask, extra_features,image.input], \n",
    "                            outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLs72DuMODJO",
    "outputId": "b115d7d1-f031-463b-8ef5-2c4d07a2f067"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 299, 299, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 299, 299, 64) 1792        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 299, 299, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 149, 149, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 149, 149, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 149, 149, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 74, 74, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 74, 74, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 74, 74, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 74, 74, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv4 (Conv2D)           (None, 74, 74, 256)  590080      block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 37, 37, 256)  0           block3_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 37, 37, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 37, 37, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 37, 37, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_2 (TFBertModel)   TFBaseModelOutputWit 133657344   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv4 (Conv2D)           (None, 37, 37, 512)  2359808     block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 255, 256)     393472      tf_bert_model_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 254, 256)     590080      tf_bert_model_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 253, 256)     786688      tf_bert_model_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 252, 256)     983296      tf_bert_model_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 18, 18, 512)  0           block4_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 255, 256)     1024        conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 254, 256)     1024        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 253, 256)     1024        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 252, 256)     1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 18, 18, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 255, 256)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 254, 256)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 253, 256)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 252, 256)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 18, 18, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1, 256)       0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 1, 256)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 1, 256)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 1, 256)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 18, 18, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 256)          0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 256)          0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 256)          0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 256)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv4 (Conv2D)           (None, 18, 18, 512)  2359808     block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1024)         0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 9, 9, 512)    0           block5_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          524800      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 81, 512)      0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 512)          2048        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 81, 512)      262656      reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 512)          0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 81, 512)      2048        dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_113 (Dropout)           (None, 512)          0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 81, 512)      0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "extra_features (InputLayer)     [(None, 17)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 81)           0           dropout_113[0][0]                \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 512)          9216        extra_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 81)           0           dot_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 512)          2048        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, 512)          0           activation_27[0][0]              \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 512)          0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 512)          0           dot_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_2 (Average)             (None, 512)          0           activation_20[0][0]              \n",
      "                                                                 dropout_113[0][0]                \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            513         average_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 157,242,689\n",
      "Trainable params: 157,237,569\n",
      "Non-trainable params: 5,120\n",
      "__________________________________________________________________________________________________\n",
      "CPU times: user 2.86 s, sys: 551 ms, total: 3.41 s\n",
      "Wall time: 3.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPOCHS          = 40\n",
    "total_steps     = len(y_train) * BATCH_SIZE\n",
    "train_data_size = len(y_train)\n",
    "steps_per_epoch = int(train_data_size / BATCH_SIZE) + 1\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "# warmup_steps    = int(num_train_steps * 0.1)\n",
    "warmup_steps    = 0\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "decay_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "      initial_learning_rate=2e-5,\n",
    "      decay_steps=num_train_steps,\n",
    "      end_learning_rate=0)\n",
    "\n",
    "warmup_schedule = nlp.optimization.WarmUp(\n",
    "        initial_learning_rate=2e-5,\n",
    "        decay_schedule_fn=decay_schedule,\n",
    "        warmup_steps=warmup_steps)\n",
    "\n",
    "optimizer       = nlp.optimization.AdamWeightDecay(\n",
    "        learning_rate=warmup_schedule,\n",
    "        epsilon=1e-8,\n",
    "        exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])\n",
    "\n",
    "#Load bert4news\n",
    "config = AutoConfig.from_pretrained('/bert4news/config.json')\n",
    "transformer = TFAutoModel.from_pretrained('/bert4news/pytorch_model.bin', from_pt=True, config=config)\n",
    "\n",
    "# transformer = TFAutoModel.from_pretrained(MODEL)\n",
    "model = create_model1(transformer, max_len=MAX_LENGTH)\n",
    "model.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics='accuracy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3LuDQbbu-3Sm",
    "outputId": "62b94e41-cf1e-48ae-bbc4-e03804d5c336"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6008249312557287, 1: 2.9795454545454545}"
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "class_weights = {i : class_weights[i] for i in range(2)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM5WbtoiUAz2"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "Q_CEl-sY4Trd"
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "class roc_auc_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred_train = self.model.predict(self.x, verbose=0)\n",
    "        roc_train = roc_auc_score(self.y, y_pred_train)\n",
    "        y_pred_val = self.model.predict(self.x_val, verbose=0)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        print('\\rroc-auc_train: %s - roc-auc_val: %s' % (str(round(roc_train,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ktez2KF-UCVT",
    "outputId": "ab5266f9-4eb3-44ca-bb19-5a8f607bb435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "roc-auc_train: 0.9214 - roc-auc_val: 0.8658                                                                                                    \n",
      "246/246 [==============================] - 258s 1s/step - loss: 0.5959 - accuracy: 0.7063 - val_loss: 0.3530 - val_accuracy: 0.8767\n",
      "Epoch 2/40\n",
      "roc-auc_train: 0.954 - roc-auc_val: 0.9271                                                                                                    \n",
      "246/246 [==============================] - 251s 1s/step - loss: 0.3787 - accuracy: 0.8360 - val_loss: 0.2634 - val_accuracy: 0.8813\n",
      "Epoch 3/40\n",
      "roc-auc_train: 0.9859 - roc-auc_val: 0.958                                                                                                    \n",
      "246/246 [==============================] - 251s 1s/step - loss: 0.2751 - accuracy: 0.8820 - val_loss: 0.2789 - val_accuracy: 0.8858\n",
      "Epoch 4/40\n",
      "roc-auc_train: 0.9936 - roc-auc_val: 0.952                                                                                                    \n",
      "246/246 [==============================] - 250s 1s/step - loss: 0.2155 - accuracy: 0.9097 - val_loss: 0.2383 - val_accuracy: 0.9041\n",
      "Epoch 5/40\n",
      "roc-auc_train: 0.9964 - roc-auc_val: 0.9583                                                                                                    \n",
      "246/246 [==============================] - 251s 1s/step - loss: 0.1890 - accuracy: 0.9151 - val_loss: 0.2049 - val_accuracy: 0.9224\n",
      "Epoch 6/40\n",
      "roc-auc_train: 0.9982 - roc-auc_val: 0.9611                                                                                                    \n",
      "246/246 [==============================] - 251s 1s/step - loss: 0.1387 - accuracy: 0.9415 - val_loss: 0.2277 - val_accuracy: 0.9178\n",
      "Epoch 7/40\n",
      "roc-auc_train: 0.9982 - roc-auc_val: 0.9639                                                                                                    \n",
      "246/246 [==============================] - 250s 1s/step - loss: 0.1364 - accuracy: 0.9385 - val_loss: 0.2696 - val_accuracy: 0.9018\n",
      "Epoch 8/40\n",
      "roc-auc_train: 0.999 - roc-auc_val: 0.9683                                                                                                    \n",
      "246/246 [==============================] - 250s 1s/step - loss: 0.1248 - accuracy: 0.9481 - val_loss: 0.1983 - val_accuracy: 0.9178\n",
      "Epoch 9/40\n",
      "roc-auc_train: 0.9997 - roc-auc_val: 0.9584                                                                                                    \n",
      "246/246 [==============================] - 251s 1s/step - loss: 0.0888 - accuracy: 0.9664 - val_loss: 0.2082 - val_accuracy: 0.9064\n",
      "Epoch 10/40\n",
      "roc-auc_train: 0.9999 - roc-auc_val: 0.9642                                                                                                    \n",
      "246/246 [==============================] - 251s 1s/step - loss: 0.0600 - accuracy: 0.9799 - val_loss: 0.2892 - val_accuracy: 0.9132\n",
      "Epoch 11/40\n",
      "roc-auc_train: 0.9991 - roc-auc_val: 0.9498                                                                                                    \n",
      "246/246 [==============================] - 251s 1s/step - loss: 0.0597 - accuracy: 0.9769 - val_loss: 0.2734 - val_accuracy: 0.9064\n",
      "Epoch 12/40\n",
      "roc-auc_train: 0.9999 - roc-auc_val: 0.9674                                                                                                    \n",
      "246/246 [==============================] - 251s 1s/step - loss: 0.0663 - accuracy: 0.9718 - val_loss: 0.2448 - val_accuracy: 0.9201\n",
      "Epoch 13/40\n",
      "roc-auc_train: 0.9997 - roc-auc_val: 0.9552                                                                                                    \n",
      "246/246 [==============================] - 251s 1s/step - loss: 0.0409 - accuracy: 0.9858 - val_loss: 0.3297 - val_accuracy: 0.9155\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "n_steps = int(np.ceil(y_train.shape[0] / BATCH_SIZE))\n",
    "\n",
    "# Checkpoint path\n",
    "ckpt_path     = f'/checkpoint/{MODEL_NAME}/'\n",
    "if not os.path.exists(ckpt_path):\n",
    "    os.makedirs(ckpt_path)\n",
    "ckpt_path     += 'cp-{epoch:02d}.h5'\n",
    "\n",
    "# Callback\n",
    "my_callbacks  = [tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_path, \n",
    "                                                    monitor='val_loss', \n",
    "                                                    save_weights_only=True,\n",
    "                                                    save_freq='epoch'),\n",
    "                 tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
    "                 roc_auc_callback(training_data=(X_train, y_train),validation_data=(X_val, y_val))]\n",
    "\n",
    "H = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    #steps_per_epoch=n_steps,\n",
    "    class_weight=class_weights,\n",
    "    shuffle=True,\n",
    "    callbacks=my_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2079Qyn8Mt8"
   },
   "source": [
    "# Load best epoch based on training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8m2qXpjMwmPH"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "EPOCHS          = 40\n",
    "total_steps     = len(y_train) * BATCH_SIZE\n",
    "train_data_size = len(y_train)\n",
    "steps_per_epoch = int(train_data_size / BATCH_SIZE) + 1\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "# warmup_steps    = int(num_train_steps * 0.1)\n",
    "warmup_steps    = 0\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "decay_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "      initial_learning_rate=2e-5,\n",
    "      decay_steps=num_train_steps,\n",
    "      end_learning_rate=0)\n",
    "\n",
    "warmup_schedule = nlp.optimization.WarmUp(\n",
    "        initial_learning_rate=2e-5,\n",
    "        decay_schedule_fn=decay_schedule,\n",
    "        warmup_steps=warmup_steps)\n",
    "\n",
    "optimizer       = nlp.optimization.AdamWeightDecay(\n",
    "        learning_rate=warmup_schedule,\n",
    "        epsilon=1e-8,\n",
    "        exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])\n",
    "\n",
    "\n",
    "#Load bert4news\n",
    "config = AutoConfig.from_pretrained('/bert4news/config.json')\n",
    "transformer = TFAutoModel.from_pretrained('/bert4news/pytorch_model.bin', from_pt=True, config=config)\n",
    "\n",
    "# transformer = TFAutoModel.from_pretrained(MODEL)\n",
    "model2      = create_model1(transformer, max_len=MAX_LENGTH)\n",
    "model2.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics='accuracy')\n",
    "    \n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "v4QPutEehi3D"
   },
   "outputs": [],
   "source": [
    "model2.load_weights(os.path.join(base_dir, f'checkpoint/{MODEL_NAME}/cp-05.h5')) #PATH to the best epoch that has highest AUC\n",
    "#model2.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CGnI1pI68Ek"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "Uyf-8Yne680u"
   },
   "outputs": [],
   "source": [
    "test_data    = os.path.join(base_dir, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hO1yKkEC6_dG",
    "outputId": "1160e5c8-5f15-4837-e700-de75cb56e407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1646, 24)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1646 entries, 0 to 1645\n",
      "Data columns (total 24 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  1646 non-null   int64  \n",
      " 1   user_name           1646 non-null   object \n",
      " 2   post_message        1646 non-null   object \n",
      " 3   timestamp_post      1646 non-null   float64\n",
      " 4   num_like_post       1646 non-null   float64\n",
      " 5   num_comment_post    1646 non-null   float64\n",
      " 6   num_share_post      1646 non-null   float64\n",
      " 7   count_chars         1646 non-null   float64\n",
      " 8   count_words         1646 non-null   float64\n",
      " 9   count_questionmark  1646 non-null   float64\n",
      " 10  count_exclaimmark   1646 non-null   float64\n",
      " 11  numHashtags         1646 non-null   float64\n",
      " 12  numUrls             1646 non-null   float64\n",
      " 13  post_month          1646 non-null   float64\n",
      " 14  post_year           1646 non-null   int64  \n",
      " 15  post_day            1646 non-null   float64\n",
      " 16  post_hour           1646 non-null   float64\n",
      " 17  post_weekday        1646 non-null   float64\n",
      " 18  cnt_fake            1646 non-null   float64\n",
      " 19  cnt_nonfake         1646 non-null   float64\n",
      " 20  ratio               1646 non-null   float64\n",
      " 21  cleaned_text        1646 non-null   object \n",
      " 22  has_title           1646 non-null   int64  \n",
      " 23  has_image           1646 non-null   int64  \n",
      "dtypes: float64(17), int64(4), object(3)\n",
      "memory usage: 308.8+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>post_message</th>\n",
       "      <th>timestamp_post</th>\n",
       "      <th>num_like_post</th>\n",
       "      <th>num_comment_post</th>\n",
       "      <th>num_share_post</th>\n",
       "      <th>count_chars</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_questionmark</th>\n",
       "      <th>count_exclaimmark</th>\n",
       "      <th>numHashtags</th>\n",
       "      <th>numUrls</th>\n",
       "      <th>post_month</th>\n",
       "      <th>post_year</th>\n",
       "      <th>post_day</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>cnt_fake</th>\n",
       "      <th>cnt_nonfake</th>\n",
       "      <th>ratio</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>has_title</th>\n",
       "      <th>has_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6015</td>\n",
       "      <td>fbf39587d668e9ae28afb00b8fc00570</td>\n",
       "      <td>Các lời khai cố tình bị rút ra để áp án tử cho...</td>\n",
       "      <td>1.590163e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.417372</td>\n",
       "      <td>-0.397499</td>\n",
       "      <td>-0.246227</td>\n",
       "      <td>-0.275266</td>\n",
       "      <td>-0.258009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528275</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.721942</td>\n",
       "      <td>0.740712</td>\n",
       "      <td>0.601962</td>\n",
       "      <td>1.290099</td>\n",
       "      <td>-0.51256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Các lời khai cố tình bị rút ra để áp án tử cho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6016</td>\n",
       "      <td>c4acc2118bdcdb5770565362b02d00e0</td>\n",
       "      <td>(NLĐO) – Lực lượng cứu hoả gần như phải thức t...</td>\n",
       "      <td>1.586500e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.410694</td>\n",
       "      <td>-0.400539</td>\n",
       "      <td>-0.246227</td>\n",
       "      <td>-0.275266</td>\n",
       "      <td>-0.258009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.231398</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.728766</td>\n",
       "      <td>-0.516311</td>\n",
       "      <td>0.601962</td>\n",
       "      <td>-0.454729</td>\n",
       "      <td>-0.51256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(nlđo) – Lực lượng cứu hoả gần như phải thức t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6017</td>\n",
       "      <td>180dd4930112053803ccea8556f33e47</td>\n",
       "      <td>Cư dân mạng đang tranh cãi sôi nổi sau quyết đ...</td>\n",
       "      <td>1.592102e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.449425</td>\n",
       "      <td>-0.443097</td>\n",
       "      <td>-0.246227</td>\n",
       "      <td>-0.275266</td>\n",
       "      <td>-0.258009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.287947</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.245197</td>\n",
       "      <td>-1.074987</td>\n",
       "      <td>1.616105</td>\n",
       "      <td>-0.454729</td>\n",
       "      <td>-0.51256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cư dân mạng đang tranh cãi sôi nổi sau quyết đ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6018</td>\n",
       "      <td>ac4d365f23909091fab2552bdc54f5ef</td>\n",
       "      <td>3 công nhân trung quốc làm cty hòa phát bị nhi...</td>\n",
       "      <td>1.596256e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.207021</td>\n",
       "      <td>-0.184710</td>\n",
       "      <td>-0.246227</td>\n",
       "      <td>-0.275266</td>\n",
       "      <td>-0.258009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.807291</td>\n",
       "      <td>2020</td>\n",
       "      <td>-1.816797</td>\n",
       "      <td>-0.795649</td>\n",
       "      <td>1.109034</td>\n",
       "      <td>-0.454729</td>\n",
       "      <td>-0.51256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3 công nhân trung quốc làm cty hòa phát bị nhi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6019</td>\n",
       "      <td>5ba5638b734e397d5263a6b1b9201abd</td>\n",
       "      <td>Một số người p Tây nhìn người Á hay Phi ko thi...</td>\n",
       "      <td>1.587462e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.309191</td>\n",
       "      <td>-0.306304</td>\n",
       "      <td>-0.246227</td>\n",
       "      <td>-0.275266</td>\n",
       "      <td>-0.258009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.231398</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.601050</td>\n",
       "      <td>-0.097303</td>\n",
       "      <td>-0.919252</td>\n",
       "      <td>-0.454729</td>\n",
       "      <td>-0.51256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Một số người p Tây nhìn người á hay Phi ko thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                         user_name  ... has_title  has_image\n",
       "0  6015  fbf39587d668e9ae28afb00b8fc00570  ...         0          0\n",
       "1  6016  c4acc2118bdcdb5770565362b02d00e0  ...         0          0\n",
       "2  6017  180dd4930112053803ccea8556f33e47  ...         0          0\n",
       "3  6018  ac4d365f23909091fab2552bdc54f5ef  ...         0          1\n",
       "4  6019  5ba5638b734e397d5263a6b1b9201abd  ...         0          1\n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test_data = pd.read_csv(test_data)\n",
    "print(df_test_data.shape)\n",
    "print(df_test_data.info())\n",
    "df_test_data[['num_like_post','num_comment_post','num_share_post']] = df_test_data[['num_like_post','num_comment_post','num_share_post']].astype(int)\n",
    "\n",
    "display(df_test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "9AFM3deG7Ajd"
   },
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "test_data_sent        = df_test_data.cleaned_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "FurQxwTmeqzT"
   },
   "outputs": [],
   "source": [
    "df_test_data[feature_columns[:-2]] = scaler.transform(df_test_data[feature_columns[:-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "5NpAoLq97EWo"
   },
   "outputs": [],
   "source": [
    "test_data_features = df_test_data[feature_columns]\n",
    "#process_features(test_data_features)\n",
    "test_data_features = test_data_features.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pol4YZ1x7Xie",
    "outputId": "949626b9-a28c-4f55-b5af-60fdc0f992d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1646 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "100%|██████████| 1646/1646 [00:00<00:00, 1650.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1646, 256), (1646, 256), (1646, 17))"
      ]
     },
     "execution_count": 106,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids       = []\n",
    "attention_masks = []\n",
    "features = []\n",
    "\n",
    "for sent in tqdm(test_data_sent):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = MAX_LENGTH,         \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'np',   \n",
    "                        truncation = True,\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "for id,_ in enumerate(test_data_features['count_chars']):\n",
    "  f = test_data_features.iloc[id].values\n",
    "  features.append(f)\n",
    "\n",
    "  \n",
    "id_test_data        = np.concatenate(input_ids)\n",
    "mask_test_data      = np.concatenate(attention_masks)\n",
    "feature_test_data   = np.array(features)\n",
    "id_test_data.shape, mask_test_data.shape, feature_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "nx6k6iXM82wF"
   },
   "outputs": [],
   "source": [
    "X_test_data         = [\n",
    "    id_test_data,\n",
    "    mask_test_data,\n",
    "    feature_test_data,\n",
    "    img_test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7wArOK-9HdO",
    "outputId": "815c93a4-a7d4-400c-aecf-1114af5f265d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 23s 435ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model2.predict(X_test_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1FO_SQWY9KGg",
    "outputId": "5109163b-024a-4f56-aaf3-f6377e57f897"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9362888 ],\n",
       "       [0.03140743],\n",
       "       [0.04029576],\n",
       "       ...,\n",
       "       [0.0084993 ],\n",
       "       [0.00566762],\n",
       "       [0.03763639]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "J_OPBhdC9kML"
   },
   "outputs": [],
   "source": [
    "file = open(\"results_model1.csv\",\"w\") \n",
    "for i in range(pred.shape[0]):\n",
    "  line = \"{0}, {1}\\n\".format(df_test_data['id'][i],pred[i][0])\n",
    "  file.write(line)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBv-uzN59nEN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "2ElsnSNUridI",
    "hTykAgaWezVa",
    "mkyubuJSOzg3",
    "O6Xo1K3HbqjV"
   ],
   "machine_shape": "hm",
   "name": "gpubert_base_vlsp2020_(1) (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
